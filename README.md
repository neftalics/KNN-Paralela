# ParalelizaciÃ³n de KNN con MPI

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![MPI](https://img.shields.io/badge/MPI-mpi4py-green.svg)](https://mpi4py.readthedocs.io/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

ImplementaciÃ³n paralela del algoritmo **K-Nearest Neighbors (KNN)** utilizando **MPI (Message Passing Interface)** en Python. Este proyecto demuestra tÃ©cnicas de paralelizaciÃ³n para algoritmos de Machine Learning mediante el modelo SPMD (Single Program, Multiple Data).

## ğŸ‘¥ Autores

- **FabiÃ¡n Alvarado Ramos**
- **Eduardo Miguel Salas Palacios**
- **NeftalÃ­ Calixto Rojas**

**InstituciÃ³n:** Proyecto Universitario de ComputaciÃ³n Paralela

---

## ğŸ“‹ DescripciÃ³n del Proyecto

El algoritmo KNN tiene una complejidad computacional de O(NÃ—M), donde N es el nÃºmero de puntos de entrenamiento y M el nÃºmero de puntos de prueba. Este proyecto paraleliza el cÃ¡lculo de distancias distribuyendo los puntos de prueba entre mÃºltiples procesos usando MPI.

### CaracterÃ­sticas Principales

- âœ… ImplementaciÃ³n secuencial de referencia
- âœ… **Tres versiones con narrativa de optimizaciÃ³n clara**:
  - **v1 (Naive P2P)**: Demuestra cuello de botella de latencia (Î±)
  - **v2 (Collective Ops)**: Optimiza ancho de banda con operaciones colectivas (Î²)
  - **v3 (Optimized)**: VectorizaciÃ³n completa para minimizar fracciÃ³n serial (f)
- âœ… Modelo maestro-trabajador con topologÃ­a DAG
- âœ… **Strong Scaling**: N fijo (1437), p variable (1, 2, 4, 8)
- âœ… AnÃ¡lisis completo de rendimiento (Speedup, Eficiencia, FLOPs)
- âœ… **ValidaciÃ³n teÃ³rica**: LogP, Amdahl, PRAM

---

## ğŸ”¬ Marco TeÃ³rico

### Modelo LogP (ComunicaciÃ³n)

El tiempo de comunicaciÃ³n en operaciones colectivas sigue el modelo LogP:

```
T_comm(p) â‰ˆ logâ‚‚(p) Ã— (Î± + NÃ—Î²)
```

donde:
- **Î±** = latencia por mensaje (segundos)
- **Î²** = tiempo por byte (segundos/byte)
- **p** = nÃºmero de procesos
- **N** = tamaÃ±o del mensaje (bytes)

**ValidaciÃ³n**: El script `fit_logp_model.py` ajusta Î± y Î² a partir de datos experimentales.

### Ley de Amdahl (Escalabilidad)

El speedup mÃ¡ximo estÃ¡ limitado por la fracciÃ³n serial:

```
Speedup_max = 1 / (f + (1-f)/p)
```

donde:
- **f** = fracciÃ³n serial del cÃ³digo (â‰ˆ 0.31 en v3)
- **p** = nÃºmero de procesos

**ImplicaciÃ³n**: Con f=0.31, el speedup mÃ¡ximo teÃ³rico es ~3.23x (incluso con infinitos procesos).

### Formalismo PRAM

El cÃ³digo v3 usa comentarios estilo PRAM (Parallel Random Access Machine):
- `BEGIN PARALLEL SECTION`: Inicio de regiÃ³n paralela
- `SYNC`: Punto de sincronizaciÃ³n
- Modelo CREW (Concurrent Read, Exclusive Write)

---

## ğŸ“– Historia de OptimizaciÃ³n

### v1: Naive Point-to-Point (LÃ­nea Base Ineficiente)

**Problema**: ComunicaciÃ³n punto a punto bloqueante en bucles.

```python
# Master envÃ­a cada punto individualmente
for worker in range(1, size):
    for test_point in X_test_chunks[worker]:
        comm.send(test_point, dest=worker)  # Alta latencia
```

**Cuello de botella**: Latencia (Î±) dominante. Cada `send`/`recv` incurre en overhead de latencia.

**Modelo de costo**: `T_comm â‰ˆ M Ã— p Ã— Î±`

### v2: Collective Operations (Mejora de ComunicaciÃ³n)

**SoluciÃ³n**: Usar operaciones colectivas MPI.

```python
# Operaciones colectivas optimizadas
X_train = comm.bcast(X_train, root=0)      # Broadcast
local_X_test = comm.scatter(X_test_chunks, root=0)  # Scatter
all_predictions = comm.gather(local_predictions, root=0)  # Gather
```

**Mejora**: ComunicaciÃ³n en Ã¡rbol logarÃ­tmico reduce latencia.

**Modelo de costo**: `T_comm â‰ˆ log(p) Ã— (Î± + NÃ—Î²)`

**LimitaciÃ³n**: AÃºn usa bucles Python para cÃ¡lculo de distancias (no vectorizado).

### v3: Final Optimized (VectorizaciÃ³n Completa)

**SoluciÃ³n**: VectorizaciÃ³n NumPy completa.

```python
# CÃ¡lculo vectorizado de distancias (sin bucles Python)
distances = np.sqrt(np.sum((X_train - test_point)**2, axis=1))
k_indices = np.argpartition(distances, k)[:k]
```

**Mejora**: Minimiza fracciÃ³n serial (f â‰ˆ 0.31).

**Optimizaciones**:
- Operaciones vectorizadas NumPy (aprovecha BLAS/LAPACK)
- `argpartition` en lugar de `argsort` (O(N) vs O(N log N))
- Timing detallado para validar Amdahl

---

## ğŸ“ Estructura del Repositorio

```
Paralela-Proyecto/
â”œâ”€â”€ src/                          # CÃ³digo fuente
â”‚   â”œâ”€â”€ knn_sequential.py         # VersiÃ³n secuencial (baseline)
â”‚   â”œâ”€â”€ v1_naive_p2p.py          # v1: ComunicaciÃ³n P2P ineficiente
â”‚   â”œâ”€â”€ v2_collective_scatter.py  # v2: Operaciones colectivas
â”‚   â”œâ”€â”€ v3_final_optimized.py     # v3: VectorizaciÃ³n completa
â”‚   â””â”€â”€ old/                      # Versiones anteriores (legacy)
â”‚
â”œâ”€â”€ scripts/                      # Scripts de experimentaciÃ³n
â”‚   â”œâ”€â”€ run_experiments.sh        # Benchmarking completo (Bash)
â”‚   â””â”€â”€ run_experiments.py        # Benchmarking completo (Python/Windows)
â”‚
â”œâ”€â”€ analysis/                     # Scripts de anÃ¡lisis
â”‚   â”œâ”€â”€ calculate_flops.py        # Calculadora de FLOPs
â”‚   â”œâ”€â”€ fit_logp_model.py         # Ajuste del modelo LogP
â”‚   â”œâ”€â”€ plot_results.py           # GrÃ¡ficas (tema oscuro)
â”‚   â””â”€â”€ plot_results_white.py     # GrÃ¡ficas (tema blanco)
â”‚
â”œâ”€â”€ docs/                         # DocumentaciÃ³n
â”‚   â”œâ”€â”€ reporte_proyecto.tex      # Reporte acadÃ©mico (LaTeX)
â”‚   â”œâ”€â”€ presentacion.html         # PresentaciÃ³n HTML
â”‚   â”œâ”€â”€ images/                   # GrÃ¡ficas tema oscuro
â”‚   â””â”€â”€ images_report/            # GrÃ¡ficas tema blanco
â”‚
â”œâ”€â”€ results_strong_scaling.csv    # Resultados de Strong Scaling
â””â”€â”€ README.md                     # Este archivo
```

---

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### Requisitos Previos

- **Python 3.8+**
- **MPI Implementation:**
  - Windows: [MS-MPI](https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi)
  - Linux/Mac: OpenMPI o MPICH

### Dependencias de Python

```bash
pip install mpi4py numpy scikit-learn matplotlib pandas
```

### Verificar InstalaciÃ³n de MPI

```bash
mpiexec --version
```

---

## ğŸ’» Uso

### 1. EjecuciÃ³n Secuencial (Baseline)

```bash
python src/knn_sequential.py
```

**Salida esperada:**
```
Accuracy (Sequential): 0.9833
Execution Time: 5.02 seconds
```

### 2. EjecuciÃ³n de Versiones Paralelas

#### v1: Naive Point-to-Point
```bash
mpiexec -n 4 python src/v1_naive_p2p.py
```

#### v2: Collective Operations
```bash
mpiexec -n 4 python src/v2_collective_scatter.py
```

#### v3: Final Optimized
```bash
mpiexec -n 4 python src/v3_final_optimized.py
```

**Salida esperada (v3 con 4 procesos):**
```
=== KNN Parallel v3 (Final Optimized - Vectorized) ===
Processes: 4
Dataset: N=1437, M=360, d=64
Theoretical FLOPs: 99,532,800 (99.53 MFLOPs)

Results:
Accuracy: 0.9833

Timing Breakdown:
Total Time:        2.09 sec
I/O Time:          0.15 sec (7.2%)
Bcast Time:        0.08 sec (3.8%)
Scatter Time:      0.02 sec (1.0%)
Compute Time:      1.75 sec (83.7%)
Gather Time:       0.01 sec (0.5%)
Total Comm Time:   0.11 sec (5.3%)

Performance Metrics:
Serial Fraction (f): 0.0718
GFLOPs/sec:          0.0569
```

### 3. Ejecutar Experimentos Completos

#### OpciÃ³n A: Script Python (Windows/Linux/Mac)
```bash
python scripts/run_experiments.py
```

#### OpciÃ³n B: Script Bash (Linux/Mac)
```bash
bash scripts/run_experiments.sh
```

**ConfiguraciÃ³n**:
- Versiones: v1, v2, v3
- Procesos: p âˆˆ {1, 2, 4, 8}
- Runs: 5 ejecuciones por configuraciÃ³n
- Output: `results_strong_scaling.csv`

### 4. AnÃ¡lisis de Resultados

#### Calcular FLOPs teÃ³ricos:
```bash
python analysis/calculate_flops.py
```

#### Ajustar modelo LogP:
```bash
python analysis/fit_logp_model.py
```

#### Generar grÃ¡ficas (tema oscuro para presentaciÃ³n):
```bash
python analysis/plot_results.py
```

#### Generar grÃ¡ficas (tema blanco para reporte):
```bash
python analysis/plot_results_white.py
```

**GrÃ¡ficas generadas:**
- `time_comparison.png` - ComparaciÃ³n de tiempos entre versiones
- `speedup_comparison.png` - Speedup vs ideal
- `efficiency_comparison.png` - Eficiencia del clÃºster
- `flops_performance.png` - Rendimiento computacional (GFLOPs/sec)
- `time_breakdown.png` - Breakdown de tiempos (v3)
- `amdahl_validation.png` - ValidaciÃ³n de Ley de Amdahl
- `logp_fit_*.png` - Ajuste del modelo LogP por versiÃ³n

---

## ğŸ“Š Resultados Principales

### Dataset Utilizado
- **Nombre:** Digits (scikit-learn)
- **CaracterÃ­sticas:** ImÃ¡genes 8Ã—8 pÃ­xeles (64 dimensiones)
- **Muestras totales:** 1797
- **Train/Test split:** 80/20 (1437 train, 360 test)
- **Clases:** 10 (dÃ­gitos 0-9)

### MÃ©tricas de Rendimiento

| Procesos | Tiempo (s) | Speedup | Eficiencia | FLOPs/s |
|----------|------------|---------|------------|---------|
| 1        | 5.02       | 1.00x   | 100%       | 19.8M   |
| 2        | 2.68       | 1.87x   | 94%        | 37.1M   |
| 4        | 2.09       | 2.40x   | 60%        | 47.6M   |
| 8        | 2.01       | 2.50x   | 31%        | 49.5M   |

### Hallazgos Clave

- âœ… **PrecisiÃ³n idÃ©ntica:** 0.9833 (secuencial vs paralelo)
- âœ… **ReducciÃ³n de tiempo:** 60% (5.02s â†’ 2.01s)
- âœ… **ConfiguraciÃ³n Ã³ptima:** 4 procesos (mejor balance speedup/eficiencia)
- âš ï¸ **SaturaciÃ³n:** A partir de 8 procesos, el overhead de comunicaciÃ³n domina
- ğŸ“ˆ **Complejidad confirmada:** O(NÂ²) verificada experimentalmente

---

## ğŸ”¬ MetodologÃ­a

### Arquitectura Maestro-Trabajador

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Main   â”‚  (Rank 0: Scatter datos)
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚
  â”Œâ”€â”€â”´â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
  â”‚     â”‚      â”‚      â”‚
â”Œâ”€â–¼â”€â” â”Œâ”€â–¼â”€â” â”Œâ”€â–¼â”€â” â”Œâ”€â–¼â”€â”
â”‚ T1â”‚ â”‚ T2â”‚ â”‚ T3â”‚ â”‚ T4â”‚  (Workers: CÃ³mputo local)
â””â”€â”¬â”€â”˜ â””â”€â”¬â”€â”˜ â””â”€â”¬â”€â”˜ â””â”€â”¬â”€â”˜
  â”‚     â”‚      â”‚      â”‚
  â””â”€â”€â”¬â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜
     â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
â”‚   End   â”‚  (Rank 0: Gather resultados)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Operaciones MPI Utilizadas

1. **`MPI_Scatter`**: Distribuir X_test entre procesos
2. **`MPI_Bcast`**: Enviar X_train, y_train a todos
3. **`MPI_Gather`**: Recolectar predicciones parciales

### CÃ¡lculo de FLOPs

Distancia euclidiana: `d(x,y) = âˆš(Î£(xáµ¢-yáµ¢)Â²)`

- **FLOPs por distancia:** 3d (d restas + d multiplicaciones + d sumas)
- **FLOPs totales:** M Ã— N Ã— 3d = 360 Ã— 1437 Ã— 192 â‰ˆ **99.5 MFLOPs**

---

## ğŸ“– DocumentaciÃ³n

### Reporte AcadÃ©mico

El reporte completo en LaTeX se encuentra en `docs/reporte_proyecto.tex` e incluye:

- âœ… IntroducciÃ³n y justificaciÃ³n
- âœ… Modelo PRAM y topologÃ­a DAG
- âœ… Desarrollo incremental (3 versiones beta)
- âœ… AnÃ¡lisis de complejidad teÃ³rica normalizada
- âœ… Resultados experimentales completos
- âœ… DerivaciÃ³n de FLOPs desde distancia euclidiana
- âœ… AnÃ¡lisis de escalabilidad
- âœ… Conclusiones y mejoras propuestas
- âœ… BibliografÃ­a con impacto descrito

### Compilar el Reporte

```bash
cd docs
pdflatex reporte_proyecto.tex
```

---

## ğŸ¯ Mejoras Futuras

1. **OptimizaciÃ³n de ComunicaciÃ³n:** Usar comunicaciÃ³n punto a punto para datasets grandes
2. **Estructuras de Datos Avanzadas:** Implementar KD-trees o Ball Trees
3. **ParalelizaciÃ³n HÃ­brida:** Combinar MPI + OpenMP
4. **Balanceo DinÃ¡mico:** DistribuciÃ³n adaptativa de carga
5. **GPU Acceleration:** Porting a CUDA/OpenCL

---

## ğŸ“š Referencias

1. **MPI Forum**. "MPI: A Message-Passing Interface Standard". Version 4.0, 2021.
2. **Gropp, W., Lusk, E., & Skjellum, A.** "Using MPI: Portable Parallel Programming with the Message-Passing Interface". MIT Press, 2014.
3. **mpi4py Documentation**. https://mpi4py.readthedocs.io/
4. **Scikit-learn**. "K-Nearest Neighbors". https://scikit-learn.org/stable/modules/neighbors.html
5. **Pacheco, P.** "An Introduction to Parallel Programming". Morgan Kaufmann, 2011.

---

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible bajo la licencia MIT.

---

## ğŸ¤ Contribuciones

Este es un proyecto acadÃ©mico. Para preguntas o sugerencias, contactar a los autores.

---

## ğŸ“ Contacto

**Repositorio:** https://github.com/neftalics/KNN-Paralela

---

**Ãšltima actualizaciÃ³n:** Noviembre 2025
